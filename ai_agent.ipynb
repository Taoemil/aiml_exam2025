{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bedc06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/tgxk0vcj1hq48nlkz0b_75540000gn/T/ipykernel_20465/1301937326.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: dict(zip(g[\"term\"], g[\"coefficient\"])))\n"
     ]
    }
   ],
   "source": [
    "# load feature importance\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_importance = pd.read_csv(\"feature_importance.csv\", delimiter=\";|,|\\t\", engine=\"python\")\n",
    "\n",
    "# clean column names\n",
    "df_importance.columns = df_importance.columns.str.strip().str.lower()\n",
    "\n",
    "# replace comma with dot and convert to float (for Danish decimal format)\n",
    "df_importance['coefficient'] = df_importance['coefficient'].astype(str).str.replace(',', '.').astype(float)\n",
    "\n",
    "# Structure as dict: {label: {term: coefficient}}\n",
    "feature_map = (\n",
    "    df_importance\n",
    "    .groupby(\"label\")\n",
    "    .apply(lambda g: dict(zip(g[\"term\"], g[\"coefficient\"])))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def get_trigger_terms(comment: str, label: str, feature_map: dict, top_n: int = 5):\n",
    "    comment_words = set(re.findall(r\"\\b\\w+\\b\", comment.lower()))\n",
    "    top_words = feature_map.get(label, {})\n",
    "    triggers = [word for word in comment_words if word in top_words]\n",
    "    sorted_triggers = sorted(triggers, key=lambda x: -top_words[x])  # highest importance first\n",
    "    return sorted_triggers[:top_n]  # limit to top-N terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0641426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 2\n",
      "python-dotenv could not parse statement starting at line 4\n",
      "python-dotenv could not parse statement starting at line 6\n",
      "python-dotenv could not parse statement starting at line 7\n",
      "python-dotenv could not parse statement starting at line 8\n",
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 2\n",
      "python-dotenv could not parse statement starting at line 4\n",
      "python-dotenv could not parse statement starting at line 6\n",
      "python-dotenv could not parse statement starting at line 7\n",
      "python-dotenv could not parse statement starting at line 8\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "# built-in\n",
    "from typing import TypedDict, Optional\n",
    "from pydantic import Field\n",
    "\n",
    "# langgraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "# local\n",
    "from src.llm import LLMCaller\n",
    "from src.model import predict_toxicity\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f4915a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 2\n",
      "python-dotenv could not parse statement starting at line 4\n",
      "python-dotenv could not parse statement starting at line 6\n",
      "python-dotenv could not parse statement starting at line 7\n",
      "python-dotenv could not parse statement starting at line 8\n"
     ]
    }
   ],
   "source": [
    "#authentication for VSC\n",
    "# load variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Read from environment\n",
    "WX_API_KEY = os.getenv(\"WX_API_KEY\")\n",
    "WX_PROJECT_ID = os.getenv(\"WX_PROJECT_ID\")\n",
    "WX_API_URL = os.getenv(\"WX_API_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7775cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm import LLMCaller\n",
    "\n",
    "model = LLMCaller(\n",
    "    api_key=WX_API_KEY,\n",
    "    project_id=WX_PROJECT_ID,\n",
    "    api_url=WX_API_URL,\n",
    "    model_id=\"watsonx/ibm/granite-3-2-8b-instruct\",\n",
    "    params={\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.5,\n",
    "        \"max_new_tokens\": 100\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f43ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar, Any\n",
    "import litellm\n",
    "from litellm.types.utils import ModelResponse, Message\n",
    "from litellm import completion\n",
    "from instructor import from_litellm, Mode\n",
    "from pydantic import BaseModel, create_model\n",
    "\n",
    "class BaseResponse(BaseModel):\n",
    "    answer: str\n",
    "\n",
    "ResponseType = TypeVar(\"ResponseType\", bound=BaseModel)\n",
    "\n",
    "class LLMCaller:\n",
    "    def __init__(self, api_key: str, project_id: str, api_url: str, model_id: str, params: dict[str, Any]):\n",
    "        self.api_key = api_key\n",
    "        self.project_id = project_id\n",
    "        self.api_url = api_url\n",
    "        self.model_id = model_id\n",
    "        self.params = params\n",
    "\n",
    "        litellm.drop_params = True\n",
    "        self.client = from_litellm(completion, mode=Mode.JSON)\n",
    "\n",
    "    def create_response_model(self, title: str, fields: dict) -> ResponseType:\n",
    "        return create_model(title, **fields, __base__=BaseResponse)\n",
    "\n",
    "    def invoke(self, prompt: str, response_model: ResponseType = BaseResponse, **kwargs) -> ResponseType:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt + f\"\\n\\nProvide your answer as an object of {type(response_model)}\"}],\n",
    "            project_id=self.project_id,\n",
    "            apikey=self.api_key,\n",
    "            api_base=self.api_url,\n",
    "            response_model=response_model,\n",
    "            **kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def chat(self, messages: list[dict[str, str] | Message], **kwargs) -> ModelResponse:\n",
    "        return completion(\n",
    "            model=self.model_id,\n",
    "            project_id=self.project_id,\n",
    "            apikey=self.api_key,\n",
    "            api_base=self.api_url,\n",
    "            messages=messages,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b02fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# load lr_cv_tuned\n",
    "model_path = \"models/lr_cv_tuned.joblib\"\n",
    "bow_model = joblib.load(model_path)\n",
    "\n",
    "def predict_toxicity(comment: str):\n",
    "    \"\"\"\n",
    "    Predict toxic labels and confidence for a comment using the loaded BoW model.\n",
    "    Also identifies trigger terms based on global feature importance.\n",
    "    \"\"\"\n",
    "    print(\"üß† [MODEL] Running lr_cv_tuned.joblib on comment:\")\n",
    "    print(f\"   \\\"{comment}\\\"\")\n",
    "\n",
    "    pred = bow_model.predict([comment])[0]       # e.g. [1, 0, 1, 0, 0, 0]\n",
    "    probas = bow_model.predict_proba([comment])  # shape: (1, 6)\n",
    "    \n",
    "    class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    toxic_labels = []\n",
    "    toxic_probs = []\n",
    "    all_triggers = []\n",
    "\n",
    "    for i, val in enumerate(pred):\n",
    "        if val == 1:\n",
    "            label = class_names[i]\n",
    "            prob = probas[0][i]\n",
    "            toxic_labels.append(label)\n",
    "            toxic_probs.append(prob)\n",
    "            print(f\"   üîπ Label predicted: {label} (probability: {prob:.2f})\")\n",
    "\n",
    "            # extract trigger terms for this label\n",
    "            comment_words = set(re.findall(r\"\\b\\w+\\b\", comment.lower()))\n",
    "            top_terms = feature_map.get(label, {})\n",
    "            triggers = [word for word in comment_words if word in top_terms]\n",
    "            triggers_sorted = sorted(triggers, key=lambda x: -top_terms[x])\n",
    "            if triggers_sorted:\n",
    "                print(f\"   üîç Trigger terms for '{label}': {', '.join(triggers_sorted[:5])}\")\n",
    "                all_triggers.extend(triggers_sorted[:5])  # limit to top 5\n",
    "\n",
    "    if toxic_labels:\n",
    "        label_str = \", \".join(toxic_labels)\n",
    "        confidence = max(toxic_probs)\n",
    "        print(f\"‚úÖ [MODEL RESULT] Predicted toxic: {label_str} (max confidence: {confidence:.2f})\")\n",
    "    else:\n",
    "        label_str = \"non-toxic\"\n",
    "        confidence = 1 - max(probas[0])\n",
    "        print(f\"‚úÖ [MODEL RESULT] Predicted non-toxic (confidence: {confidence:.2f})\")\n",
    "\n",
    "    return label_str, confidence, list(set(all_triggers))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb12d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional\n",
    "from pydantic import Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "# define State for the agent\n",
    "class CommentState(TypedDict):\n",
    "    comment: str\n",
    "    label: Optional[str]\n",
    "    confidence: Optional[float]\n",
    "    explanation: Optional[str]\n",
    "    verbose: bool\n",
    "\n",
    "# define Node Functions\n",
    "def run_model(state: CommentState):\n",
    "    print(\"\\nüöÄ [AGENT NODE] Running model node...\")\n",
    "    label, confidence, triggers = predict_toxicity(state[\"comment\"])\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence,\n",
    "        \"triggers\": triggers\n",
    "    }\n",
    "\n",
    "def explain_prediction(state: CommentState):\n",
    "    prompt = f\"\"\"\n",
    "    A machine learning model classified the following comment as '{state['label']}' with {state['confidence']:.2f} confidence.\n",
    "\n",
    "    Comment: \"{state['comment']}\"\n",
    "\n",
    "    Please explain step-by-step why this label is appropriate.\n",
    "    Mention specific words or tone that could influence the model.\n",
    "    \"\"\"\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"explanation\": response.answer}\n",
    "\n",
    "def display_result(state: CommentState):\n",
    "    if state[\"verbose\"]:\n",
    "        print(\"=\"*60)\n",
    "        print(f\"üìù Comment: {state['comment']}\")\n",
    "        print(f\"üì£ Prediction: {state['label'].upper()} ({state['confidence']:.2f} confidence)\")\n",
    "        print(f\"üîç Explanation:\\n{state['explanation']}\")\n",
    "        print(\"=\"*60)\n",
    "    return {}\n",
    "\n",
    "# create the LangGraph\n",
    "graph = StateGraph(CommentState)\n",
    "graph.add_node(\"run_model\", run_model)\n",
    "graph.add_node(\"explain\", explain_prediction)\n",
    "graph.add_node(\"display\", display_result)\n",
    "\n",
    "graph.add_edge(START, \"run_model\")\n",
    "graph.add_edge(\"run_model\", \"explain\")\n",
    "graph.add_edge(\"explain\", \"display\")\n",
    "graph.add_edge(\"display\", END)\n",
    "\n",
    "compiled_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a97c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ [AGENT NODE] Running model node...\n",
      "üß† [MODEL] Running lr_cv_tuned.joblib on comment:\n",
      "   \"I‚Äôm not trying to get banned, but sleezy fucktard is the phrase that comes to mind.\"\n",
      "‚úÖ [MODEL RESULT] Predicted non-toxic (confidence: 0.95)\n",
      "============================================================\n",
      "üìù Comment: I‚Äôm not trying to get banned, but sleezy fucktard is the phrase that comes to mind.\n",
      "üì£ Prediction: NON-TOXIC (0.95 confidence)\n",
      "üîç Explanation:\n",
      "The comment is classified as 'non-toxic' due to the following reasons:\n",
      "\n",
      "1. Contextual understanding: The model likely understands the context in which the phrase 'sleezy fucktard' is used. In this case, it is used to express frustration or disdain towards a specific individual, rather than as a direct insult or attack on a group or individual's characteristics.\n",
      "\n",
      "2. Tone and intent: The comment begins with 'I‚Äôm not trying to get banned,' which indicates the user's awareness of community guidelines and their intention not to violate them. This tone suggests a level of self-restraint and consideration, which can influence the model's interpretation.\n",
      "\n",
      "3. Lack of personal attack: The phrase 'sleezy fucktard' is directed at a specific individual, rather than a generalized attack on a group or individual's characteristics. This focus on a single person, without intent to harm or demean, can contribute to the model's 'non-toxic' classification.\n",
      "\n",
      "4.Confidence level: The model's 0.95 confidence in its classification suggests a high degree of certainty in its assessment. This confidence level is based on the model's training and ability to recognize patterns indicative of toxicity or non-toxicity.\n",
      "\n",
      "Specific words or tone that could influence the model: The use of strong language like 'sleezy fucktard' can be seen as toxic in many contexts. However, the model likely considers the context, intent, and lack of generalized attack when making its classification. The opening statement about not wanting to get banned further influences the model's interpretation by emphasizing the user's awareness and adherence to community guidelines.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# example comments\n",
    "sample_comment = \"wtf is grandpa talking about?  Get him his medicine and put him to bed!\" #True toxcic from reddit predictions\n",
    "sample_comment1 = \"Playing for Real Madrid really seems to inflate egos massively doesn't it?\" #True non-toxic from reddit predictions\n",
    "sample_comment2 = \"I‚Äôm not trying to get banned, but sleezy fucktard is the phrase that comes to mind.\" #False negative\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# small delay, since ibm has an limit for how many request i can send in 8 seconds\n",
    "time.sleep(1.0)\n",
    "\n",
    "result = compiled_graph.invoke({\n",
    "    \"comment\": sample_comment2,\n",
    "    \"verbose\": True\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my-project)",
   "language": "python",
   "name": "exam_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
