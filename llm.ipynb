{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf16f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet ibm-watsonx-ai python-dotenv tqdm scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports & auth setup\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "# WatsonX \n",
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffa0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .env for key\n",
    "load_dotenv()\n",
    "api_key    = os.getenv(\"WATSONX_API_KEY\") or getpass(\"IBM Cloud API key: \")\n",
    "url        = os.getenv(\"WATSONX_URL\")\n",
    "project_id = os.getenv(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "# Build client\n",
    "creds  = Credentials(url=url, api_key=api_key)\n",
    "client = APIClient(credentials=creds, project_id=project_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8933e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# — Authentication & client setup (run this first!) —\n",
    "import os\n",
    "from getpass import getpass\n",
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "\n",
    "# 1) Prompt (or load) your API key\n",
    "api_key    = getpass(\"IBM Cloud API key: \")\n",
    "url        = \"https://us-south.ml.cloud.ibm.com\"   # or from os.environ\n",
    "project_id = \"e92879d6-e36f-4374-b05f-98a704b287f2\"\n",
    "\n",
    "# 2) Build Credentials + Client\n",
    "creds  = Credentials(url=url, api_key=api_key)\n",
    "client = APIClient(credentials=creds, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443946c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate LLM \n",
    "params = TextGenParameters(\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=64,\n",
    "    stop_sequences=[\"}\"]\n",
    ")\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb01fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load & clean data\n",
    "train_df = pd.read_csv(\"sample_data/train.csv\")\n",
    "test_df  = pd.read_csv(\"sample_data/test.csv\")\n",
    "tl_df    = pd.read_csv(\"sample_data/test_labels.csv\")\n",
    "\n",
    "label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"https?://\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(clean_text)\n",
    "test_df['comment_text']  = test_df['comment_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7231585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation subset\n",
    "# sample for prompt tuning\n",
    "sample_df = train_df.sample(n=5000, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to scored subset\n",
    "scored = tl_df[label_cols].ne(-1).all(axis=1)\n",
    "scored_ids = tl_df.loc[scored, 'id']\n",
    "eval_df    = test_df[test_df['id'].isin(scored_ids)].reset_index(drop=True)\n",
    "y_true     = tl_df.set_index('id').loc[eval_df['id'], label_cols].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651178ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JSON-schema prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a toxicity classifier.  Given the comment under TEXT, return ONLY a JSON object\n",
    "with these six keys (true or false values):\n",
    "  {keys}\n",
    "\n",
    "Example:\n",
    "TEXT: \"You are an idiot.\"\n",
    "{{\"toxic\": true, \"severe_toxic\": false, \"obscene\": false, \n",
    "  \"threat\": false, \"insult\": true, \"identity_hate\": false}}\n",
    "\n",
    "Now classify this comment:\n",
    "TEXT: {text}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# pre-format the keys snippet\n",
    "KEYS_JSON = json.dumps({k: False for k in label_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with small samples for decreased runtimes\n",
    "eval_small = eval_df.sample(frac=0.02, random_state=42).reset_index(drop=True)\n",
    "print(f\"Running LLM on {len(eval_small)} / {len(eval_df)} comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b56b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inference loop over slice:\n",
    "preds = []\n",
    "for txt in tqdm(eval_small['comment_text'], desc=\"LLM classify (25%)\"):\n",
    "    prompt = SYSTEM_PROMPT.format(keys=KEYS_JSON, text=txt)\n",
    "    resp   = model.generate(prompt)\n",
    "    raw    = resp['results'][0]['generated_text'].strip()\n",
    "\n",
    "    if '{' in raw and '}' in raw:\n",
    "        raw = raw[raw.find('{'): raw.rfind('}')+1]\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        if not isinstance(parsed, dict):\n",
    "            raise ValueError(\"Not a dict\")\n",
    "        d = parsed\n",
    "    except Exception:\n",
    "        d = {k: False for k in label_cols}\n",
    "\n",
    "    preds.append([int(bool(d.get(k, False))) for k in label_cols])\n",
    "\n",
    "y_pred_small = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bfea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "y_true_small = tl_df.set_index('id').loc[eval_small['id'], label_cols].values.astype(int)\n",
    "print(classification_report(y_true_small, y_pred_small, target_names=label_cols, zero_division=0))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"ROC-AUC per label:\")\n",
    "for i, lbl in enumerate(label_cols):\n",
    "    auc = roc_auc_score(y_true_small[:, i], y_pred_small[:, i])\n",
    "    print(f\"  {lbl:15s}: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my-project)",
   "language": "python",
   "name": "exam_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
