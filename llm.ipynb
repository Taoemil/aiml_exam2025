{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf16f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet ibm-watsonx-ai python-dotenv tqdm scikit-learn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports & auth setup\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "# WatsonX \n",
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffa0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#authentication for VSC\n",
    "# .env for key\n",
    "load_dotenv()\n",
    "api_key    = os.getenv(\"WATSONX_API_KEY\") or getpass(\"IBM Cloud API key: \")\n",
    "url        = os.getenv(\"WATSONX_URL\")\n",
    "project_id = os.getenv(\"WATSONX_PROJECT_ID\")\n",
    "\n",
    "# Build client\n",
    "creds  = Credentials(url=url, api_key=api_key)\n",
    "client = APIClient(credentials=creds, project_id=project_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8933e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# — Authentication for google colab\n",
    "import os\n",
    "from getpass import getpass\n",
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "\n",
    "# 1) Prompt (or load) your API key\n",
    "api_key    = getpass(\"IBM Cloud API key: \")\n",
    "url        = \"https://us-south.ml.cloud.ibm.com\"\n",
    "project_id = \"e92879d6-e36f-4374-b05f-98a704b287f2\"\n",
    "\n",
    "creds  = Credentials(url=url, api_key=api_key)\n",
    "client = APIClient(credentials=creds, project_id=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443946c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate LLM \n",
    "params = TextGenParameters(\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=64,\n",
    "    stop_sequences=[\"}\"]\n",
    ")\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb01fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load & clean data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df  = pd.read_csv(\"test.csv\")\n",
    "tl_df    = pd.read_csv(\"test_labels.csv\")\n",
    "\n",
    "label_cols = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"https?://\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(clean_text)\n",
    "test_df['comment_text']  = test_df['comment_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7231585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation subset\n",
    "# sample for prompt tuning\n",
    "sample_df = train_df.sample(n=5000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to scored subset\n",
    "scored = tl_df[label_cols].ne(-1).all(axis=1)\n",
    "scored_ids = tl_df.loc[scored, 'id']\n",
    "eval_df    = test_df[test_df['id'].isin(scored_ids)].reset_index(drop=True)\n",
    "y_true     = tl_df.set_index('id').loc[eval_df['id'], label_cols].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651178ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON‐schema snippet:\n",
    "KEYS_JSON = json.dumps({k: False for k in label_cols})\n",
    "\n",
    "#JSON-schema prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a toxicity classifier. You will:\n",
    "1) Reason step by step whether the text is toxic, obscene, a threat, insult, etc.\n",
    "2) Then output ONLY the final JSON object with these six keys (true or false):\n",
    "\n",
    "{keys}\n",
    "\n",
    "Examples:\n",
    "\n",
    "TEXT: \"COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\"\n",
    "THOUGHTS: Strong profanity, personal attack → toxic, severe_toxic, obscene, insult.\n",
    "OUTPUT: {{\"toxic\": true, \"severe_toxic\": true, \"obscene\": true, \"threat\": false, \"insult\": true, \"identity_hate\": false}}\n",
    "\n",
    "TEXT: \"Stupid piece of shit! Stop deleting my stuff, asshole, go die and fall in a hole!\"\n",
    "THOUGHTS: Very insulting, obscene, but no direct threat → toxic, severe_toxic, obscene, insult.\n",
    "OUTPUT: {{\"toxic\": true, \"severe_toxic\": true, \"obscene\": true, \"threat\": false, \"insult\": true, \"identity_hate\": false}}\n",
    "\n",
    "TEXT: \"You are gay or antisemitian?\"\n",
    "THOUGHTS: Hate slur → toxic, obscene, insult, identity_hate.\n",
    "OUTPUT: {{\"toxic\": true, \"severe_toxic\": false, \"obscene\": true, \"threat\": false, \"insult\": true, \"identity_hate\": true}}\n",
    "\n",
    "TEXT: \"Hi! I am back again! Last warning! Stop undoing my edits or die\"\n",
    "THOUGHTS: “or die” is a threat → toxic + threat.\n",
    "OUTPUT: {{\"toxic\": true, \"severe_toxic\": false, \"obscene\": false, \"threat\": true, \"insult\": false, \"identity_hate\": false}}\n",
    "\n",
    "TEXT: \"FUCK YOUR FILTHY MOTHER IN THE ASS\"\n",
    "THOUGHTS: Profanity + insult → toxic, obscene, insult.\n",
    "OUTPUT: {{\"toxic\": true, \"severe_toxic\": false, \"obscene\": true, \"threat\": false, \"insult\": true, \"identity_hate\": false}}\n",
    "\n",
    "Now classify this comment:\n",
    "TEXT: {text}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with small samples for decreased runtimes\n",
    "eval_small = eval_df.sample(frac=0.02, random_state=42).reset_index(drop=True)\n",
    "print(f\"Running LLM on {len(eval_small)} / {len(eval_df)} comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b56b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preds = []\n",
    "for txt in tqdm(eval_small['comment_text'], desc=\"LLM classify\"):\n",
    "    prompt = SYSTEM_PROMPT.format(keys=KEYS_JSON, text=txt)\n",
    "    resp   = model.generate(prompt)\n",
    "    raw    = resp['results'][0]['generated_text'].strip()\n",
    "\n",
    "    if '{' in raw and '}' in raw:\n",
    "        raw = raw[raw.find('{'): raw.rfind('}')+1]\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        if not isinstance(parsed, dict):\n",
    "            raise ValueError(\"Not a dict\")\n",
    "        d = parsed\n",
    "    except Exception:\n",
    "        d = {k: False for k in label_cols}\n",
    "\n",
    "    preds.append([int(bool(d.get(k, False))) for k in label_cols])\n",
    "\n",
    "y_pred_small = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bfea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "y_true_small = tl_df.set_index('id').loc[eval_small['id'], label_cols].values.astype(int)\n",
    "print(classification_report(y_true_small, y_pred_small, target_names=label_cols, zero_division=0))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"ROC-AUC per label:\")\n",
    "for i, lbl in enumerate(label_cols):\n",
    "    auc = roc_auc_score(y_true_small[:, i], y_pred_small[:, i])\n",
    "    print(f\"  {lbl:15s}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9461eb",
   "metadata": {},
   "source": [
    "# ---- APPENDIX: single‐label “toxic” evaluation ----\n",
    "Here i try only using one label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa68659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot:\n",
    "KEYS_SINGLE_JSON = json.dumps({\"toxic\": False})\n",
    "SYSTEM_PROMPT_SINGLE = \"\"\"\n",
    "You are a toxicity classifier. You will:\n",
    "1) Reason step by step whether the text is toxic, obscene, a threat, insult, etc.\n",
    "2) Then output ONLY the final JSON object with this key (true or false):\n",
    "{keys}\n",
    "\n",
    "Examples:\n",
    "\n",
    "TEXT: \"COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\"\n",
    "THOUGHTS: Strong profanity, personal attack → toxic.\n",
    "OUTPUT: {{\"toxic\": true}}\n",
    "\n",
    "TEXT: \"Stupid piece of shit! Stop deleting my stuff, asshole, go die and fall in a hole!\"\n",
    "THOUGHTS: Very insulting, obscene, → toxic.\n",
    "OUTPUT: {{\"toxic\": true}}\n",
    "\n",
    "TEXT: \"You are gay or antisemitian?\"\n",
    "THOUGHTS: Hate slur → toxic.\n",
    "OUTPUT: {{\"toxic\": true}}\n",
    "\n",
    "TEXT: \"Hi! I am back again! Last warning! Stop undoing my edits or die\"\n",
    "THOUGHTS: “or die” is a threat → toxic.\n",
    "OUTPUT: {{\"toxic\": true}}\n",
    "\n",
    "TEXT: \"FUCK YOUR FILTHY MOTHER IN THE ASS\"\n",
    "THOUGHTS: Profanity + insult → toxic.\n",
    "OUTPUT: {{\"toxic\": true.}}\n",
    "\n",
    "TEXT: \"I love cats!\"\n",
    "THOUGHTS: No abusive or harassing content → non-toxic.\n",
    "OUTPUT: {{\"toxic\": false}}\n",
    "\n",
    "TEXT: \"I'm sorry to say this, but I have to fail this article's GAN. The several major problems that  brought up in this article's previous GA assessment have not been issued\"\n",
    "THOUGHTS: No abusive or harassing content → non-toxic.\n",
    "OUTPUT: {{\"toxic\": false}}\n",
    "\n",
    "\n",
    "Now classify this comment:\n",
    "TEXT: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Few shot + chain of thought\n",
    "KEYS_SINGLE_JSON = json.dumps({\"toxic\": False})\n",
    "SYSTEM_PROMPT_SINGLE_CHAIN = \"\"\"\n",
    "You are a toxicity classifier. You will:\n",
    "1) Reason step by step whether the text is toxic, obscene, a threat, insult, etc.\n",
    "2) Then output ONLY the final JSON object with this key (true or false):\n",
    "{keys}\n",
    "\n",
    "Examples:\n",
    "\n",
    "TEXT: \"COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\"\n",
    "THOUGHTS: Strong profanity, personal attack → toxic.\n",
    "OUTPUT: {{\"toxic\": true}}\n",
    "\n",
    "TEXT: \"Stupid piece of shit! Stop deleting my stuff, asshole, go die and fall in a hole!\"\n",
    "THOUGHTS: Very insulting, obscene, → toxic.\n",
    "OUTPUT: {{\"toxic\": true}}\n",
    "\n",
    "TEXT: \"You are gay or antisemitian?\"\n",
    "THOUGHTS: Hate slur → toxic.\n",
    "OUTPUT: {{\"toxic\": true}}\n",
    "\n",
    "TEXT: \"Hi! I am back again! Last warning! Stop undoing my edits or die\"\n",
    "THOUGHTS: “or die” is a threat → toxic.\n",
    "OUTPUT: {{\"toxic\": true}}\n",
    "\n",
    "TEXT: \"FUCK YOUR FILTHY MOTHER IN THE ASS\"\n",
    "THOUGHTS: Profanity + insult → toxic.\n",
    "OUTPUT: {{\"toxic\": true.}}\n",
    "\n",
    "TEXT: \"I love cats!\"\n",
    "THOUGHTS: No abusive or harassing content → non-toxic.\n",
    "OUTPUT: {{\"toxic\": false}}\n",
    "\n",
    "TEXT: \"I'm sorry to say this, but I have to fail this article's GAN. The several major problems that  brought up in this article's previous GA assessment have not been issued\"\n",
    "THOUGHTS: No abusive or harassing content → non-toxic.\n",
    "OUTPUT: {{\"toxic\": false}}\n",
    "\n",
    "\n",
    "Now classify this comment:\n",
    "TEXT: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b80116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LLM on eval_small comments (same subset you already sampled):\n",
    "preds_single = []\n",
    "for txt in tqdm(eval_small['comment_text'], desc=\"LLM classify (toxic-only)\"):\n",
    "    prompt = SYSTEM_PROMPT_SINGLE_CHAIN.format(keys=KEYS_SINGLE_JSON, text=txt)\n",
    "    resp   = model.generate(prompt)\n",
    "    raw    = resp['results'][0]['generated_text'].strip()\n",
    "    \n",
    "    # extract the JSON\n",
    "    if '{' in raw and '}' in raw:\n",
    "        raw = raw[raw.find('{'): raw.rfind('}')+1]\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        toxic_flag = bool(parsed.get(\"toxic\", False))\n",
    "    except Exception:\n",
    "        toxic_flag = False\n",
    "    \n",
    "    preds_single.append(int(toxic_flag))\n",
    "\n",
    "y_pred_toxic = np.array(preds_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels and metrics for just “toxic”:\n",
    "y_true_toxic = tl_df.set_index('id')\\\n",
    "                    .loc[eval_small['id'], \"toxic\"]\\\n",
    "                    .astype(int)\\\n",
    "                    .values\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(\"=== Single-label ‘toxic’ ===\")\n",
    "print(classification_report(y_true_toxic, y_pred_toxic, target_names=[\"non-toxic\",\"toxic\"], zero_division=0))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_true_toxic, y_pred_toxic):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my-project)",
   "language": "python",
   "name": "exam_code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
